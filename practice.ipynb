{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1d3a236-1cd9-4831-8c43-b8e5f14bcd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/baijingyuan/jupyterPj/pytorch_practice'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fedf6913-ea7f-45e7-a9d9-47db7e26fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3de41d72-766d-4586-8a40-b37a3e8d96ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "465db6fa-ac11-4cf3-a652-0ca4cb6aab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "352bb956-2eba-4730-94e8-ccb010518946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58b30dd2-e590-4547-af6b-d180e6a4b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fdc55f1-7a67-41c4-87ae-1f289f3ae233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4874, 0.9415, 0.6074],\n",
      "        [0.4554, 0.9872, 0.6349],\n",
      "        [0.2027, 0.6128, 0.9628],\n",
      "        [0.8822, 0.8551, 0.2986],\n",
      "        [0.1383, 0.3929, 0.4397]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b7c18eb-5262-4df4-8c2c-2029b860e1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2123, 0.1424, 0.1502],\n",
      "        [0.7859, 0.5134, 0.7885],\n",
      "        [0.4988, 0.0781, 0.9173],\n",
      "        [0.7660, 0.1304, 0.4404],\n",
      "        [0.2574, 0.6179, 0.4238]]) tensor([[0.4936, 0.9348, 0.8879],\n",
      "        [0.9479, 0.5654, 0.1303],\n",
      "        [0.6349, 0.1140, 0.6328],\n",
      "        [0.3526, 0.0639, 0.7017],\n",
      "        [0.9563, 0.2016, 0.5413]]) tensor([[0.7059, 1.0773, 1.0381],\n",
      "        [1.7338, 1.0788, 0.9187],\n",
      "        [1.1337, 0.1921, 1.5501],\n",
      "        [1.1186, 0.1943, 1.1421],\n",
      "        [1.2137, 0.8195, 0.9651]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "result = torch.add(x, y)\n",
    "print(x, y, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "673d8ee4-797d-4659-a562-f898eb1d38ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fbfb5d3-0e38-4580-99a4-a180c0e9517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fbe2be8-ced2-4697-aaba-9c515ca86ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([5, 3])\n",
    "torch.tensor([5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edcc9951-8a7c-46f8-b8a6-841d19c38aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(5, 3) \n",
    "x = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0ff71e0-29d7-40af-a791-34ec0945e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4559, 0.7120, 0.2219],\n",
      "        [0.2581, 0.1372, 0.0854],\n",
      "        [0.1462, 0.2865, 0.2870],\n",
      "        [0.5127, 0.7986, 0.5871],\n",
      "        [0.1493, 0.5187, 0.3689]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a26c70d-9480-4891-8290-b1c2e8ff3e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3891, -0.3139,  0.8295],\n",
      "        [ 0.3435,  1.2583, -0.9577],\n",
      "        [-0.1483,  0.9781, -1.8480],\n",
      "        [-0.2281,  0.7332,  1.1610],\n",
      "        [ 0.5940,  0.7403,  0.1029]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ddcf4e6-12df-4775-9aae-815c8864a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.FloatTensor([5, 3])\n",
    "y = torch.IntTensor([5.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62da05fe-0f22-4143-897b-0c4cd25cc890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ebdfde9-1db7-411c-ba70-88fb65a41672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2dfda30e-ab38-4264-a80d-2b33e42a4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61d39c0e-85a9-43a0-8c4e-993eafef9461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6032,  1.2109, -0.9783],\n",
      "        [ 1.4730, -0.4425,  0.1360],\n",
      "        [ 0.1464,  1.5923, -0.0047],\n",
      "        [-1.7136,  0.8701, -1.9218],\n",
      "        [-0.1205,  1.9350, -0.5295]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1527e-6258-4b37-8cb9-a03dd5be2157",
   "metadata": {},
   "source": [
    "randn   均值为0, 方差为1, float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f5e6384-09ff-43f2-900b-e81414508c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(0, 12).reshape(3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8aab0eb6-4624-4090-89a0-005b76a72210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d719615b-dcb0-43d1-9b20-c6e9f8b8bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18d19102-5d1d-437a-b2d0-b92908d2929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef05a21d-ac32-4165-b6bc-969a3822fd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8420, -0.5736, -0.3009],\n",
       "        [-2.4455, -1.6021,  0.8904]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a909b3de-1a6a-4d73-aee1-574e61d4d05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8420, 0.5736, 0.3009],\n",
       "        [2.4455, 1.6021, 0.8904]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.abs(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd6d1217-ee7d-480e-8cb6-2fbe567dbe54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1239, 0.5943, 0.9745],\n",
       "        [0.4128, 0.6702, 0.9311]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b9be28b-6f2b-40cf-a70c-3bbe6b51f5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2208, 0.4712, 0.8697],\n",
       "        [0.4506, 0.2116, 0.1988]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a797aa80-4432-41de-80f2-b23ed98eb5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3447, 1.0654, 1.8442],\n",
       "        [0.8635, 0.8819, 1.1298]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03b6583f-0fb0-4e09-a282-7e3f291c1ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.1239, 10.5943, 10.9745],\n",
       "        [10.4128, 10.6702, 10.9311]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.add(x, 10)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3006943-ffdb-4af4-8123-331f0c3947b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3, 4, 5, 6]).reshape(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1703a6ff-ecd4-4db8-be2b-bd8d04ccccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.clamp(a, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa3910b9-ecc7-4f43-bcec-a85323be2f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3],\n",
       "        [3, 3, 3]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddfd6ab6-3d79-4477-af34-af53d2b13ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.clamp(a, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cedd0ac9-d6a3-40c0-96cd-0219b1b38895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3],\n",
       "        [4, 4, 4]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dda50296-dae0-4a06-a7dd-f0d2f9f44ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.6667, 1.0000],\n",
       "        [1.0000, 1.2500, 1.5000]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.div(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bd2fe48-990f-4ccf-8bd6-ba45a578defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e70c4b1-63fc-436e-bccb-9db0037e4d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2, 3, 4])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83173d57-1404-4889-981d-3d52e721013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  9, 16])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.pow(a, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7efb1d9-127a-4429-88d0-791ff9cb6157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2, 3, 4])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87cd43c7-a200-4965-86c1-25658f0438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "238a85af-2b2b-4576-b7d1-4d4c06d5c6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  9, 16])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2126e472-48f4-4910-b9f6-0ea2042e2d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 3, 4]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3],[2, 3, 4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8870e8ba-3362-4f34-8b6b-2a54953fc3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[2, 3, 4],[1, 1, 1]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "04950d63-ee91-4283-ab1b-d88a95933214",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.mm(a, b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eee71371-ed2f-4773-8b96-a3bad2704db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20,  6],\n",
       "        [29,  9]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6620e985-7c15-4e61-8d74-902074b6ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5711,  0.5093, -0.7649],\n",
       "        [ 1.1576,  1.6526, -0.5625]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1764edf-bd6d-4245-b293-d83ae1e942bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4192, -0.8984, -0.1498])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "883d1e61-ffe7-454c-a542-78dbbac720e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5824, -0.9151])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.mv(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a00a76b-a0c5-4a92-8653-28934c482bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里用torch.nn实现一个MLP\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim1, hid_dim2, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "          nn.Linear(in_dim, hid_dim1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hid_dim1, hid_dim2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(hid_dim2, out_dim),\n",
    "          nn.ReLU()\n",
    "       )\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217636d-aaee-4a2f-a25a-25049599732b",
   "metadata": {},
   "source": [
    "# 简易神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0480a50f-9caf-4de7-9ee8-3be183f11125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "epoch:0,loss:51473864.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:1,loss:112936728.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:2,loss:441915904.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:3,loss:831945088.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:4,loss:51561628.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:5,loss:20461974.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:6,loss:11566545.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:7,loss:7435552.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:8,loss:5144543.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:9,loss:3745824.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:10,loss:2840056.2500\n",
      "torch.Size([100, 100])\n",
      "epoch:11,loss:2228726.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:12,loss:1803439.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:13,loss:1499883.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:14,loss:1278300.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:15,loss:1113128.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:16,loss:987286.0625\n",
      "torch.Size([100, 100])\n",
      "epoch:17,loss:889374.5625\n",
      "torch.Size([100, 100])\n",
      "epoch:18,loss:811546.1250\n",
      "torch.Size([100, 100])\n",
      "epoch:19,loss:748356.3125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch_n = 100#一个批次输入数据的数量\n",
    "hidden_layer = 100\n",
    "input_data = 1000#每个数据的特征为1000\n",
    "output_data = 10\n",
    "\n",
    "x = torch.randn(batch_n,input_data)\n",
    "y = torch.randn(batch_n,output_data)\n",
    "\n",
    "\n",
    "w1 = torch.randn(input_data,hidden_layer)\n",
    "w2 = torch.randn(hidden_layer,output_data)\n",
    "\n",
    "epoch_n = 20\n",
    "lr = 1e-6\n",
    "\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    h1=x.mm(w1)#(100,1000)*(1000,100)-->100*100\n",
    "    print(h1.shape)\n",
    "    h1=h1.clamp(min=0)\n",
    "    y_pred = h1.mm(w2)\n",
    "    \n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    print(\"epoch:{},loss:{:.4f}\".format(epoch,loss))\n",
    "    \n",
    "    grad_y_pred = 2*(y_pred-y)\n",
    "    grad_w2 = h1.t().mm(grad_y_pred)\n",
    "    \n",
    "    grad_h = grad_y_pred.clone()\n",
    "    grad_h = grad_h.mm(w2.t())\n",
    "    grad_h.clamp_(min=0)#将小于0的值全部赋值为0，相当于sigmoid\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 = w1 -lr*grad_w1\n",
    "    w2 = w2 -lr*grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e38a7e-5ab4-42d4-9fe6-617290516d3e",
   "metadata": {},
   "source": [
    "# 输入层 (X，100*1000维特征)\n",
    "    |\n",
    "    | 线性变换 (权重 W1 1000*100层)\n",
    "    v\n",
    "# 隐藏层线性输出 (H1_linear 100*100)\n",
    "    |\n",
    "    | 激活函数 (ReLU min=0)\n",
    "    v\n",
    "# 隐藏层激活输出 (H1  100*100)\n",
    "    |\n",
    "    | 线性变换 (权重 W2 100*10)\n",
    "    v\n",
    "# 输出层预测值 (Y_pred 100*10)\n",
    "    |\n",
    "    | 损失函数 (如均方误差  sigma(y_pred - y))\n",
    "    v\n",
    "# 损失值 (Loss)\n",
    "\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6506539-f4ab-483c-ae42-b384ee6b1976",
   "metadata": {},
   "source": [
    "# 损失值 (Loss)\n",
    "    ^\n",
    "    | 计算损失对预测输出 Y_pred 的梯度\n",
    "    |\n",
    "# 输出层预测值 (Y_pred)\n",
    "    ^\n",
    "    | 计算损失对 W2 和 H1 的梯度\n",
    "    |\n",
    "# 隐藏层激活输出 (H1)\n",
    "    ^\n",
    "    | 计算激活函数 (ReLU) 的梯度\n",
    "    |\n",
    "# 隐藏层线性输出 (H1_linear)\n",
    "    ^\n",
    "    | 计算损失对 W1 的梯度\n",
    "    |\n",
    "# 输入层 (X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2486a1d5-c575-4fe7-93dd-f9dfed2880a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7912, -0.7973, -0.9928],\n",
       "        [ 1.9184,  1.7247,  0.0890]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e4a2cef-3dc8-40b5-b647-6378a1ea8ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6542,  1.5756,  1.2641],\n",
       "        [-0.9960, -0.2708, -1.0823]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "48893b22-bc8d-4ff5-83c0-ec7139a73c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2094, -0.5060, -0.7854],\n",
       "        [-1.9261, -6.3698, -0.0822]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.div(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39afebf-46ca-42cc-999b-6d0f2df96c07",
   "metadata": {},
   "source": [
    "# 完整神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6de8b83a-64f5-4c6a-b112-303f15e7ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "epoch:0,loss:39410624.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:1,loss:50610140.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:2,loss:190803104.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:3,loss:725850880.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:4,loss:487000256.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:5,loss:6520564.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:6,loss:4930673.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:7,loss:3880887.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:8,loss:3144816.0000\n",
      "torch.Size([100, 100])\n",
      "epoch:9,loss:2606876.2500\n",
      "torch.Size([100, 100])\n",
      "epoch:10,loss:2200566.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:11,loss:1885770.6250\n",
      "torch.Size([100, 100])\n",
      "epoch:12,loss:1636482.7500\n",
      "torch.Size([100, 100])\n",
      "epoch:13,loss:1435995.6250\n",
      "torch.Size([100, 100])\n",
      "epoch:14,loss:1272471.1250\n",
      "torch.Size([100, 100])\n",
      "epoch:15,loss:1137558.2500\n",
      "torch.Size([100, 100])\n",
      "epoch:16,loss:1024735.8750\n",
      "torch.Size([100, 100])\n",
      "epoch:17,loss:929558.6250\n",
      "torch.Size([100, 100])\n",
      "epoch:18,loss:848454.7500\n",
      "torch.Size([100, 100])\n",
      "epoch:19,loss:778858.7500\n",
      "torch.Size([100, 100])\n",
      "epoch:20,loss:718584.3125\n",
      "torch.Size([100, 100])\n",
      "epoch:21,loss:666022.4375\n",
      "torch.Size([100, 100])\n",
      "epoch:22,loss:619829.3125\n",
      "torch.Size([100, 100])\n",
      "epoch:23,loss:578915.1250\n",
      "torch.Size([100, 100])\n",
      "epoch:24,loss:542521.3750\n",
      "torch.Size([100, 100])\n",
      "epoch:25,loss:509926.1875\n",
      "torch.Size([100, 100])\n",
      "epoch:26,loss:480548.3125\n",
      "torch.Size([100, 100])\n",
      "epoch:27,loss:453986.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:28,loss:429838.5000\n",
      "torch.Size([100, 100])\n",
      "epoch:29,loss:407778.0625\n",
      "torch.Size([100, 100])\n",
      "epoch:30,loss:387559.7500\n",
      "torch.Size([100, 100])\n",
      "epoch:31,loss:368937.0625\n",
      "torch.Size([100, 100])\n",
      "epoch:32,loss:351741.1250\n",
      "torch.Size([100, 100])\n",
      "epoch:33,loss:335805.9375\n",
      "torch.Size([100, 100])\n",
      "epoch:34,loss:321005.3438\n",
      "torch.Size([100, 100])\n",
      "epoch:35,loss:307176.9062\n",
      "torch.Size([100, 100])\n",
      "epoch:36,loss:294275.6250\n",
      "torch.Size([100, 100])\n",
      "epoch:37,loss:282190.8438\n",
      "torch.Size([100, 100])\n",
      "epoch:38,loss:270864.0312\n",
      "torch.Size([100, 100])\n",
      "epoch:39,loss:260235.4219\n",
      "torch.Size([100, 100])\n",
      "epoch:40,loss:250224.6562\n",
      "torch.Size([100, 100])\n",
      "epoch:41,loss:240794.7344\n",
      "torch.Size([100, 100])\n",
      "epoch:42,loss:231899.7500\n",
      "torch.Size([100, 100])\n",
      "epoch:43,loss:223495.9531\n",
      "torch.Size([100, 100])\n",
      "epoch:44,loss:215544.6250\n",
      "torch.Size([100, 100])\n",
      "epoch:45,loss:208014.2031\n",
      "torch.Size([100, 100])\n",
      "epoch:46,loss:200872.7812\n",
      "torch.Size([100, 100])\n",
      "epoch:47,loss:194093.1875\n",
      "torch.Size([100, 100])\n",
      "epoch:48,loss:187649.1562\n",
      "torch.Size([100, 100])\n",
      "epoch:49,loss:181518.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100#一个批次输入数据的数量\n",
    "hidden_layer = 100\n",
    "input_data = 1000#每个数据的特征为1000\n",
    "output_data = 10\n",
    "\n",
    "x = Variable(torch.randn(batch_n,input_data),requires_grad=False)\n",
    "y = Variable(torch.randn(batch_n,output_data),requires_grad=False)\n",
    "#用Variable对Tensor数据类型变量进行封装的操作。requires_grad如果是False，表示该变量在进行自动梯度计算的过程中不会保留梯度值。\n",
    "w1 = Variable(torch.randn(input_data,hidden_layer),requires_grad=True)\n",
    "w2 = Variable(torch.randn(hidden_layer,output_data),requires_grad=True)\n",
    "\n",
    "#学习率和迭代次数\n",
    "epoch_n=50\n",
    "lr=1e-6\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    h1=x.mm(w1)#(100,1000)*(1000,100)-->100*100\n",
    "    print(h1.shape)\n",
    "    h1=h1.clamp(min=0)\n",
    "    y_pred = h1.mm(w2)\n",
    "    #y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    print(\"epoch:{},loss:{:.4f}\".format(epoch,loss.data))\n",
    "    \n",
    "#     grad_y_pred = 2*(y_pred-y)\n",
    "#     grad_w2 = h1.t().mm(grad_y_pred)\n",
    "    loss.backward()#后向传播\n",
    "#     grad_h = grad_y_pred.clone()\n",
    "#     grad_h = grad_h.mm(w2.t())\n",
    "#     grad_h.clamp_(min=0)#将小于0的值全部赋值为0，相当于sigmoid\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    w1.data -= lr*w1.grad.data\n",
    "    w2.data -= lr*w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    \n",
    "#     w1 = w1 -lr*grad_w1\n",
    "#     w2 = w2 -lr*grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cda2b-ad44-4fac-8a6a-a721c63eae9d",
   "metadata": {},
   "source": [
    "# 自定义传播函数 (重写前向函数与后向函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "844153fa-e05f-4520-90ea-7ef63b5ccdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,loss:35758028.0000\n",
      "epoch:1,loss:39210404.0000\n",
      "epoch:2,loss:46610216.0000\n",
      "epoch:3,loss:47317268.0000\n",
      "epoch:4,loss:35344444.0000\n",
      "epoch:5,loss:18168670.0000\n",
      "epoch:6,loss:7505094.5000\n",
      "epoch:7,loss:3350727.0000\n",
      "epoch:8,loss:1971500.5000\n",
      "epoch:9,loss:1436996.7500\n",
      "epoch:10,loss:1156504.5000\n",
      "epoch:11,loss:967096.6250\n",
      "epoch:12,loss:821854.1250\n",
      "epoch:13,loss:704703.7500\n",
      "epoch:14,loss:608131.7500\n",
      "epoch:15,loss:527575.0625\n",
      "epoch:16,loss:459903.4688\n",
      "epoch:17,loss:402616.8750\n",
      "epoch:18,loss:353769.6250\n",
      "epoch:19,loss:312068.5000\n",
      "epoch:20,loss:276184.8438\n",
      "epoch:21,loss:245141.6562\n",
      "epoch:22,loss:218210.7500\n",
      "epoch:23,loss:194745.2031\n",
      "epoch:24,loss:174233.6250\n",
      "epoch:25,loss:156243.5156\n",
      "epoch:26,loss:140416.3906\n",
      "epoch:27,loss:126449.9141\n",
      "epoch:28,loss:114104.5703\n",
      "epoch:29,loss:103143.5781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 64#一个批次输入数据的数量\n",
    "hidden_layer = 100\n",
    "input_data = 1000#每个数据的特征为1000\n",
    "output_data = 10\n",
    "class Model(torch.nn.Module):#完成类继承的操作\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()#类的初始化\n",
    "        \n",
    "    def forward(self,input,w1,w2):\n",
    "        x = torch.mm(input,w1)  # h1\n",
    "        x = torch.clamp(x,min = 0) # active function\n",
    "        x = torch.mm(x,w2)      # output\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "model = Model()\n",
    "x = Variable(torch.randn(batch_n,input_data),requires_grad=False)\n",
    "y = Variable(torch.randn(batch_n,output_data),requires_grad=False)\n",
    "#用Variable对Tensor数据类型变量进行封装的操作。requires_grad如果是F，表示该变量在进行自动梯度计算的过程中不会保留梯度值。\n",
    "w1 = Variable(torch.randn(input_data,hidden_layer),requires_grad=True)\n",
    "w2 = Variable(torch.randn(hidden_layer,output_data),requires_grad=True)\n",
    "\n",
    "epoch_n=30\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    y_pred = model(x,w1,w2)\n",
    "    \n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    print(\"epoch:{},loss:{:.4f}\".format(epoch,loss.data))\n",
    "    loss.backward()\n",
    "    w1.data -= lr*w1.grad.data\n",
    "    w2.data -= lr*w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f989e5-1de2-41f3-bc81-0b427c7bae6d",
   "metadata": {},
   "source": [
    "# torch.nn.Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1f07b0c-d213-41c5-ab96-7043b296c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)   # 不保留梯度\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)  # 不保留梯度\n",
    "models = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_data, hidden_layer),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_layer, output_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd45080-912e-4a4e-990c-fc1613b0f126",
   "metadata": {},
   "source": [
    "# models = torch.nn.Sequential() 括号内是搭建的神经网络模型的结构, \n",
    "# 参数会按照定义好的序列传递\n",
    "# \n",
    "# torch.nn.Linear() 做线性运算, 输入特征数, 输出特征数, 是否使用偏置, 默认为True\n",
    "# \n",
    "# torch.nn.ReLU() 激活函数, 默认定义不需要参数\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9de8ff-f793-4c7c-83de-1c1fc8174944",
   "metadata": {},
   "source": [
    "# torch.nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9dab19e0-9ce2-404c-bac9-993cb7b38625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0406)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.MSELoss()\n",
    "x = Variable(torch.randn(100, 100))\n",
    "y = Variable(torch.randn(100, 100))\n",
    "loss = loss_f(x, y)\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600234d-661e-4a96-8789-595bc782af47",
   "metadata": {},
   "source": [
    "# torch.nn.MSELoss()使用均方误差函数对损失值计算, 定义不需要传入参数, 使用实例时输入两个维度一样的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a11b0-33bf-4fe9-90b3-8a2c9916f121",
   "metadata": {},
   "source": [
    "# torch.nn.L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99acc1ae-f5fa-474e-801d-ce0e7085d5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1345)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.L1Loss()\n",
    "x = Variable(torch.randn(100, 100))\n",
    "y = Variable(torch.randn(100, 100))\n",
    "loss = loss_f(x, y)\n",
    "loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647007e-e425-4632-a932-84a44c724abb",
   "metadata": {},
   "source": [
    "# torch.nn.MSELoss()使用平均绝对误差函数对损失值计算, 定义不需要传入参数, 使用实例时输入两个维度一样的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4109cbcf-b460-4fea-8582-ac3ccf0a24a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:1.0164\n",
      "epoch:1000, loss:1.0156\n",
      "epoch:2000, loss:1.0148\n",
      "epoch:3000, loss:1.0140\n",
      "epoch:4000, loss:1.0132\n",
      "epoch:5000, loss:1.0124\n",
      "epoch:6000, loss:1.0116\n",
      "epoch:7000, loss:1.0108\n",
      "epoch:8000, loss:1.0100\n",
      "epoch:9000, loss:1.0092\n",
      "epoch:10000, loss:1.0084\n",
      "epoch:11000, loss:1.0077\n",
      "epoch:12000, loss:1.0069\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.MSELoss()\n",
    "x = Variable(torch.randn(100, 100))\n",
    "y = Variable(torch.randn(100, 100))\n",
    "loss = loss_f(x, y)\n",
    "\n",
    "\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "\n",
    "models = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_data, hidden_layer),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_layer, output_data)\n",
    ")\n",
    "\n",
    "epoch_n=12345\n",
    "lr = 1e-6\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(epoch_n):\n",
    "    y_pred = models(x)\n",
    "\n",
    "    loss = loss_f(y_pred, y)\n",
    "    if epoch%1000 == 0:\n",
    "        print(\"epoch:{}, loss:{:.4f}\".format(epoch, loss.data))\n",
    "    models.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for param in models.parameters():\n",
    "        param.data -= param.grad.data*lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ab65b-b5c8-46db-9fd1-7b63167437b8",
   "metadata": {},
   "source": [
    "# torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef18ca1e-38c1-4c1b-82fc-e3e8756f6a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.1271\n",
      "Epoch:1, Loss:1.1249\n",
      "Epoch:2, Loss:1.1227\n",
      "Epoch:3, Loss:1.1205\n",
      "Epoch:4, Loss:1.1183\n",
      "Epoch:5, Loss:1.1161\n",
      "Epoch:6, Loss:1.1139\n",
      "Epoch:7, Loss:1.1117\n",
      "Epoch:8, Loss:1.1095\n",
      "Epoch:9, Loss:1.1073\n",
      "Epoch:10, Loss:1.1052\n",
      "Epoch:11, Loss:1.1030\n",
      "Epoch:12, Loss:1.1008\n",
      "Epoch:13, Loss:1.0987\n",
      "Epoch:14, Loss:1.0965\n",
      "Epoch:15, Loss:1.0944\n",
      "Epoch:16, Loss:1.0922\n",
      "Epoch:17, Loss:1.0901\n",
      "Epoch:18, Loss:1.0880\n",
      "Epoch:19, Loss:1.0858\n",
      "Epoch:20, Loss:1.0837\n",
      "Epoch:21, Loss:1.0816\n",
      "Epoch:22, Loss:1.0795\n",
      "Epoch:23, Loss:1.0774\n",
      "Epoch:24, Loss:1.0753\n",
      "Epoch:25, Loss:1.0732\n",
      "Epoch:26, Loss:1.0711\n",
      "Epoch:27, Loss:1.0690\n",
      "Epoch:28, Loss:1.0669\n",
      "Epoch:29, Loss:1.0649\n",
      "Epoch:30, Loss:1.0628\n",
      "Epoch:31, Loss:1.0607\n",
      "Epoch:32, Loss:1.0587\n",
      "Epoch:33, Loss:1.0566\n",
      "Epoch:34, Loss:1.0546\n",
      "Epoch:35, Loss:1.0525\n",
      "Epoch:36, Loss:1.0505\n",
      "Epoch:37, Loss:1.0485\n",
      "Epoch:38, Loss:1.0464\n",
      "Epoch:39, Loss:1.0444\n",
      "Epoch:40, Loss:1.0424\n",
      "Epoch:41, Loss:1.0404\n",
      "Epoch:42, Loss:1.0384\n",
      "Epoch:43, Loss:1.0364\n",
      "Epoch:44, Loss:1.0344\n",
      "Epoch:45, Loss:1.0324\n",
      "Epoch:46, Loss:1.0304\n",
      "Epoch:47, Loss:1.0284\n",
      "Epoch:48, Loss:1.0264\n",
      "Epoch:49, Loss:1.0244\n",
      "Epoch:50, Loss:1.0224\n",
      "Epoch:51, Loss:1.0205\n",
      "Epoch:52, Loss:1.0185\n",
      "Epoch:53, Loss:1.0166\n",
      "Epoch:54, Loss:1.0146\n",
      "Epoch:55, Loss:1.0126\n",
      "Epoch:56, Loss:1.0107\n",
      "Epoch:57, Loss:1.0087\n",
      "Epoch:58, Loss:1.0068\n",
      "Epoch:59, Loss:1.0049\n",
      "Epoch:60, Loss:1.0029\n",
      "Epoch:61, Loss:1.0010\n",
      "Epoch:62, Loss:0.9991\n",
      "Epoch:63, Loss:0.9972\n",
      "Epoch:64, Loss:0.9953\n",
      "Epoch:65, Loss:0.9933\n",
      "Epoch:66, Loss:0.9914\n",
      "Epoch:67, Loss:0.9895\n",
      "Epoch:68, Loss:0.9876\n",
      "Epoch:69, Loss:0.9857\n",
      "Epoch:70, Loss:0.9838\n",
      "Epoch:71, Loss:0.9819\n",
      "Epoch:72, Loss:0.9801\n",
      "Epoch:73, Loss:0.9782\n",
      "Epoch:74, Loss:0.9763\n",
      "Epoch:75, Loss:0.9744\n",
      "Epoch:76, Loss:0.9726\n",
      "Epoch:77, Loss:0.9707\n",
      "Epoch:78, Loss:0.9688\n",
      "Epoch:79, Loss:0.9670\n",
      "Epoch:80, Loss:0.9651\n",
      "Epoch:81, Loss:0.9633\n",
      "Epoch:82, Loss:0.9614\n",
      "Epoch:83, Loss:0.9596\n",
      "Epoch:84, Loss:0.9578\n",
      "Epoch:85, Loss:0.9559\n",
      "Epoch:86, Loss:0.9541\n",
      "Epoch:87, Loss:0.9523\n",
      "Epoch:88, Loss:0.9505\n",
      "Epoch:89, Loss:0.9486\n",
      "Epoch:90, Loss:0.9468\n",
      "Epoch:91, Loss:0.9450\n",
      "Epoch:92, Loss:0.9432\n",
      "Epoch:93, Loss:0.9414\n",
      "Epoch:94, Loss:0.9396\n",
      "Epoch:95, Loss:0.9378\n",
      "Epoch:96, Loss:0.9360\n",
      "Epoch:97, Loss:0.9343\n",
      "Epoch:98, Loss:0.9325\n",
      "Epoch:99, Loss:0.9307\n",
      "Epoch:100, Loss:0.9289\n",
      "Epoch:101, Loss:0.9272\n",
      "Epoch:102, Loss:0.9254\n",
      "Epoch:103, Loss:0.9237\n",
      "Epoch:104, Loss:0.9219\n",
      "Epoch:105, Loss:0.9202\n",
      "Epoch:106, Loss:0.9184\n",
      "Epoch:107, Loss:0.9167\n",
      "Epoch:108, Loss:0.9150\n",
      "Epoch:109, Loss:0.9132\n",
      "Epoch:110, Loss:0.9115\n",
      "Epoch:111, Loss:0.9098\n",
      "Epoch:112, Loss:0.9081\n",
      "Epoch:113, Loss:0.9064\n",
      "Epoch:114, Loss:0.9047\n",
      "Epoch:115, Loss:0.9030\n",
      "Epoch:116, Loss:0.9013\n",
      "Epoch:117, Loss:0.8996\n",
      "Epoch:118, Loss:0.8979\n",
      "Epoch:119, Loss:0.8962\n",
      "Epoch:120, Loss:0.8945\n",
      "Epoch:121, Loss:0.8928\n",
      "Epoch:122, Loss:0.8912\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "loss_f = torch.nn.MSELoss()\n",
    "x = Variable(torch.randn(100, 100))\n",
    "y = Variable(torch.randn(100, 100))\n",
    "loss = loss_f(x, y)\n",
    "\n",
    "\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(batch_n, input_data), requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n, output_data), requires_grad = False)\n",
    "\n",
    "models = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_data, hidden_layer),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_layer, output_data)\n",
    ")\n",
    "\n",
    "epoch_n=123\n",
    "lr = 1e-5\n",
    "loss_f = torch.nn.MSELoss()\n",
    "\n",
    "optimzer = torch.optim.Adam(models.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "for epoch in range(epoch_n):\n",
    "    y_pred = models(x)\n",
    "    loss = loss_f(y_pred, y)\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss.data))\n",
    "    optimzer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimzer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdb8a1-fcf9-4ace-85c6-8db272158399",
   "metadata": {},
   "source": [
    "# 搭建神经网络实现手写数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a59ceaf1-04a1-4984-bbed-03cd98d88156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    " ])   \n",
    "data_train = datasets.MNIST(root = \"./data/\",\n",
    "                           transform = transform,\n",
    "                           train = True,\n",
    "                           download = True)\n",
    "\n",
    "data_test = datasets.MNIST(root = \"./data/\",\n",
    "                           transform = transform,\n",
    "                           train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbcd80-30a7-43cc-8fdc-61eb7705d465",
   "metadata": {},
   "source": [
    "# torch.transforms\n",
    "torch.transforms中有大量数据变换类，如：\n",
    "\n",
    "# torchvision.transforms.Resize\n",
    "用于对载入的图片数据按照我们需求的大小进行缩放。传递的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。h代表高度，w代表宽度，如果输入的是整型数据那么h和w都等于这个数。\n",
    "\n",
    "# torchvision.transforms.Scale\n",
    "用于对载入的图片数据按照我们需求的大小进行缩放。和Resize类似。\n",
    "\n",
    "# torchvision.transforms.CenterCrop\n",
    "用于对载入的图片以图片中心为参考点，按照我们需要的大小进行裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。\n",
    "\n",
    "# torchvision.transforms.RandomCrop\n",
    "用于对载入的图片按照我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于(h,w)的序列。\n",
    "\n",
    "# torchvision.transforms.RandomHorizontalFlip\n",
    "用于对载入的图片按随机概率进行水平翻转。我们通过传递给这个类的自定义随机概率，如果没有定义，则使用默认的概率为0.5\n",
    "\n",
    "# torchvision.transforms.RandomVerticalFlip\n",
    "用于对载入的图片按随机概率进行垂直翻转。我们通过传递给这个类的自定义随机概率，如果没有定义，则使用默认的概率为0.5\n",
    "\n",
    "# torchvision.transforms.ToTensor\n",
    "用于对载入的图片数据进行类型转换，将之前构成PIL图片数据转换为Tensor数据类型的变量，让PyTorch能够对其进行计算和处理。\n",
    "\n",
    "# torchvision.transforms.ToPILImage:\n",
    "用于对Tensor变量的数据转换成PIL图片数据，主要为方便图片显示。\r",
    "543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1731c6a7-9c8e-4a72-9c66-5151d79c0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.transforms: \n",
    "transform=transforms.Compose(\n",
    "    [transforms.ToTensor(),#将PILImage转换为张量\n",
    "     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))#将[0, 1]归一化到[-1, 1]\n",
    "     #前面的（0.5，0.5，0.5） 是 R G B 三个通道上的均值， 后面(0.5, 0.5, 0.5)是三个通道的标准差\n",
    "    ])\n",
    "#上述代码我们可以将transforms.Compose()看作一种容器，它能够同时对多种数据变换进行组合。\n",
    "#传入的参数是一个列表，列表中的元素就是对载入数据进行的变换操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b0343-db6c-451d-8c66-0eb1399ffaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；\n",
    "# transform=transforms.Compose(\n",
    "#     [transforms.ToTensor(),#将PILImage转换为张量\n",
    "#      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))#将[0, 1]归一化到[-1, 1]\n",
    "#      #前面的（0.5，0.5，0.5） 是 R G B 三个通道上的均值， 后面(0.5, 0.5, 0.5)是三个通道的标准差\n",
    "#     ])\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    " ])   # 修改的位置\n",
    "\n",
    "data_train = datasets.MNIST(root=\"./data/\",\n",
    "                           transform=transform,\n",
    "                           train = True,\n",
    "                           download = True)\n",
    "data_test = datasets.MNIST(root=\"./data/\",\n",
    "                          transform = transform,\n",
    "                          train = False)\n",
    "\n",
    "data_loader_train=torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                       batch_size=64,#每个batch载入的图片数量，默认为1,这里设置为64\n",
    "                                        shuffle=True,\n",
    "                                        #num_workers=2#载入训练数据所需的子任务数\n",
    "                                       )\n",
    "data_loader_test=torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                      batch_size=64,\n",
    "                                      shuffle=True)\n",
    "                                      #num_workers=2)\n",
    "\n",
    "#预览\n",
    "#在尝试过多次之后，发现错误并不是这一句引发的，而是因为图片格式是灰度图只有一个channel，需要变成RGB图才可以，所以将其中一行做了修改：\n",
    "images,labels = next(iter(data_loader_train))\n",
    "# dataiter = iter(data_loader_train) #随机从训练数据中取一些数据\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "img = torchvision.utils.make_grid(images)\n",
    "\n",
    "img = img.numpy().transpose(1,2,0)\n",
    "std = [0.5,0.5,0.5]\n",
    "mean = [0.5,0.5,0.5]\n",
    "img = img*std+mean\n",
    "print([labels[i] for i in range(64)])\n",
    "plt.imshow(img)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #构建卷积层之后的全连接层以及分类器\n",
    "        self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(stride=2,kernel_size=2)\n",
    "                )\n",
    "        \n",
    "        self.dense = torch.nn.Sequential(\n",
    "                nn.Linear(14*14*128,1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(1024,10)\n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=x.view(-1,14*14*128)\n",
    "        x=self.dense(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "cost = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "print(model)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    print(\"Epoch {}/{}\".format(epoch,n_epochs))\n",
    "    print(\"-\"*10)\n",
    "    for data in data_loader_train:\n",
    "        X_train,y_train = data\n",
    "        X_train,y_train = Variable(X_train),Variable(y_train)\n",
    "        outputs = model(X_train)\n",
    "        _,pred=torch.max(outputs.data,1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = cost(outputs,y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data\n",
    "        running_correct += torch.sum(pred == y_train.data)\n",
    "    testing_correct = 0\n",
    "    for data in data_loader_test:\n",
    "        X_test,y_test = data\n",
    "        X_test,y_test = Variable(X_test),Variable(y_test)\n",
    "        outputs = model(X_test)\n",
    "        _,pred=torch.max(outputs.data,1)\n",
    "        testing_correct += torch.sum(pred == y_test.data)\n",
    "    print(\"Loss is:{:4f},Train Accuracy is:{:.4f}%,Test Accuracy is:{:.4f}\".format(running_loss/len(data_train),100*running_correct/len(data_train)\n",
    "                                                                                  ,100*testing_correct/len(data_test)))\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset=data_test,\n",
    "                                              batch_size = 4,\n",
    "                                              shuffle = True)\n",
    "X_test,y_test = next(iter(data_loader_test))\n",
    "inputs = Variable(X_test)\n",
    "pred = model(inputs)\n",
    "_,pred = torch.max(pred,1)\n",
    "\n",
    "print(\"Predict Label is:\",[i for i in pred.data])\n",
    "print(\"Real Label is:\",[i for i in y_test])\n",
    "img = torchvision.utils.make_grid(X_test)\n",
    "img = img.numpy().transpose(1,2,0)\n",
    "\n",
    "std = [0.5,0.5,0.5]\n",
    "mean = [0.5,0.5,0.5]\n",
    "img = img*std+mean\n",
    "plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc6284-55f7-4e8d-ba1c-1838dbe963a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
